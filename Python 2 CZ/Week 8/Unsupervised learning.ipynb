{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Týden 8. Strojové učení - pokračování.\n",
    "\n",
    "## Naivní Bayes\n",
    "\n",
    "Klasifikátor Naivní Bayes je založen na Bayesově teorému a předpokládá podmíněnou nezávislost mezi rysy danými označením třídy. Je široce používán pro klasifikační úlohy, jako je filtrování spamu, analýza sentimentu a další.\n",
    "\n",
    "Bayesova věta je formulována jako:\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "Pro klasifikační problém s rysy $ \\mathbf{x} = (x_1, x_2, \\ldots, x_n) $ a značkou třídy $ y $ je cílem najít:\n",
    "\n",
    "$$\n",
    "P(y|\\mathbf{x}) = \\frac{P(\\mathbf{x}|y) \\times P(y)}{P(\\mathbf{x})}\n",
    "$$\n",
    "\n",
    "V Naivním Bayesovi vycházíme z naivního předpokladu, že všechny rysy jsou podmíněně nezávislé vzhledem k $ y $, tedy:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{x}|y) = P(x_1|y) \\times P(x_2|y) \\times \\ldots \\times P(x_n|y)\n",
    "$$\n",
    "\n",
    "Předpovědí je pak třída $ y $, která maximalizuje $ P(y|\\mathbf{x}) $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V tomto kódu se make_blobs používá k vytvoření syntetické datové sady se dvěma shluky. Na těchto datech je natrénován Gaussův model Naive Bayes (GaussianNB). K vizualizaci rozhodovací hranice je vytvořena mřížka meshgrid, která je zobrazena pomocí contourf.\n",
    "\n",
    "Barevné oblasti představují předpovězenou třídu pro každou oblast prostoru příznaků a rozptýlené body jsou původní datové body, zbarvené podle jejich skutečné značky třídy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data\n",
    "X, y = make_blobs(n_samples=300, centers=2, random_state=42, cluster_std=2.0)\n",
    "\n",
    "# Train Gaussian Naive Bayes model\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X, y)\n",
    "\n",
    "# Create mesh grid for visualization\n",
    "xx, yy = np.meshgrid(np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 100),\n",
    "                     np.linspace(X[:, 1].min() - 1, X[:, 1].max() + 1, 100))\n",
    "\n",
    "# Predict classes for each grid point\n",
    "Z = gnb.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o', linewidth=1)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Gaussian Naive Bayes Decision Boundary')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rozhodovací strom\n",
    "\n",
    "Rozhodovací strom je stromová struktura podobná diagramu, kde vnitřní uzel představuje funkci, větev představuje rozhodovací pravidlo a každý listový uzel představuje výsledek. Jedná se o algoritmus učení pod dohledem, který se používá pro klasifikační i regresní úlohy.\n",
    "\n",
    "Základní algoritmus pro sestavení rozhodovacího stromu je založen na rekurzivním rozdělení množiny dat na podmnožiny. K rozhodnutí o rozdělení lze použít různá kritéria, například informační zisk, Giniho nečistotu nebo snížení rozptylu. \n",
    "\n",
    "Informační zisk pro binární klasifikaci lze například vyjádřit jako:\n",
    "\n",
    "$$\n",
    "\\text{Informační zisk} = \\text{Entropie(rodič)} - \\sum \\left( \\frac{n}{N} \\times \\text{Entropie(podřízený)} \\right)\n",
    "$$\n",
    "\n",
    "Kde \\( \\text{Entropie}(S) \\) pro množinu \\( S \\) obsahující \\( p \\) kladných případů a \\( n \\) záporných případů je definováno jako:\n",
    "\n",
    "$$\n",
    "\\text{Entropie}(S) = -p\\log_2(p) - n\\log_2(n)\n",
    "$$\n",
    "\n",
    "### Kroky algoritmu\n",
    "1. Vyberte atribut ze souboru dat.\n",
    "2. Vypočítejte významnost atributu při rozdělení dat.\n",
    "3. Rozdělte data na základě vybraného atributu.\n",
    "4. Opakujte kroky 1-3 pro každou větev, dokud není splněna jedna z podmínek zastavení, například maximální hloubka, minimum vzorků na listu nebo práh nejlepšího rozdělení.\n",
    "\n",
    "\n",
    "Níže je uveden kód k demonstraci rozhodovacího stromu na syntetické sadě dat s vizualizací rozhodovací hranice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create synthetic data\n",
    "X, y = make_moons(n_samples=100, noise=0.2, random_state=42)\n",
    "\n",
    "# Initialize Decision Tree and fit data\n",
    "clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Create mesh grid for visualization\n",
    "xx, yy = np.meshgrid(np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 100),\n",
    "                     np.linspace(X[:, 1].min() - 1, X[:, 1].max() + 1, 100))\n",
    "\n",
    "# Predict classes for each grid point\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o', linewidth=1)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Decision Tree Classifier')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V tomto příkladu je pomocí funkce `make_moons` vytvořena syntetická datová sada a rozhodovací strom (`DecisionTreeClassifier`) je natrénován s maximální hloubkou 2. Rozhodovací hranice je spolu s původními datovými body vizualizována. Každá barevná oblast v grafu představuje předpovězenou třídu pro danou oblast prostoru příznaků.\n",
    "\n",
    "## Náhodný les\n",
    "Náhodný les je metoda skupinového učení, která funguje tak, že vytváří množství rozhodovacích stromů. Čím více stromů je v \"lese\", tím je model robustnější. Náhodný les lze použít pro klasifikační i regresní úlohy.\n",
    "\n",
    "Náhodný les se oproti jednomu rozhodovacímu stromu zlepšuje zprůměrováním nebo většinovým hlasováním jednotlivých stromů. Matematicky je pro klasifikaci konečná předpověď $ \\hat{y} $ dána vztahem:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{mode}(\\hat{y}_1, \\hat{y}_2, \\ldots, \\hat{y}_n)\n",
    "$$\n",
    "\n",
    "kde $ n $ je počet stromů a $ \\hat{y}_i $ je předpověď ze stromu $ i^{th} $. U regrese je to průměr výstupů jednotlivých stromů.\n",
    "\n",
    "### Kroky algoritmu\n",
    "\n",
    "1. Náhodně vyberte \"k\" prvků z celkem \"m\" prvků, kde $ k < m $.\n",
    "2. Z \"k\" rysů vypočítejte uzel pomocí nejlepšího bodu rozdělení.\n",
    "3. Rozdělte uzel na podřízené uzly pomocí nejlepšího rozdělení.\n",
    "4. Opakujte kroky 1-3, dokud listové uzly neobsahují méně než předem stanovený počet vzorků nebo nedosáhnou přijatelné čistoty.\n",
    "5. Sestavte \"n\" stromů pomocí kroků 1-4 a seskupte je tak, aby vytvořily les."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize Random Forest Classifier and fit data\n",
    "clf = RandomForestClassifier(n_estimators=20, random_state=42)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Create mesh grid for visualization\n",
    "xx, yy = np.meshgrid(np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 100),\n",
    "                     np.linspace(X[:, 1].min() - 1, X[:, 1].max() + 1, 100))\n",
    "\n",
    "# Predict classes for each grid point\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o', linewidth=1)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Random Forest Classifier')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V tomto příkladu je na syntetických datech natrénován RandomForestClassifier s 20 stromy (n_estimators=20). Hranice rozhodování je pak vizualizována stejným způsobem jako u jednoduchého rozhodovacího stromu.\n",
    "\n",
    "Na rozdíl od jednoduchého rozhodovacího stromu využívá Random Forest ke konečné klasifikaci většinový hlas ze všech jednotlivých stromů. Jak je vidět, vede to k pružnější a robustnější rozhodovací hranici.\n",
    "\n",
    "## Strojové učení bez dohledu\n",
    "\n",
    "Strojové učení bez dohledu je typ strojového učení, při kterém se algoritmus učí z neoznačených dat. Na rozdíl od učení pod dohledem, kdy je každému trénovacímu příkladu přiřazen štítek, v případě učení bez dohledu pracuje model sám na sobě, aby odhalil přirozenou strukturu dat. Hlavním cílem je často modelovat rozložení dat tak, aby bylo možné tento model použít pro úlohy, jako třeba:\n",
    "\n",
    "Shlukování: Algoritmy jako K-means, hierarchické shlukování a DBSCAN vytvářejí skupiny nebo \"shluky\" na základě přirozeného rozložení dat.\n",
    "\n",
    "Snížení dimenzionality: Techniky, jako je analýza hlavních komponent (PCA) nebo autoenkodéry, snižují počet uvažovaných náhodných proměnných, přičemž zachovávají podstatné vlastnosti.\n",
    "\n",
    "Asociace: Algoritmy jako Apriori a FP-growth se používají k nalezení pravidel, která popisují vztahy mezi zdánlivě nezávislými položkami v daném souboru dat.\n",
    "\n",
    "Detekce anomálií: Algoritmy jako Isolation Forest a One-Class SVM se používají k detekci abnormálních vzorů, které neodpovídají očekávanému chování.\n",
    "\n",
    "## K-means clustering\n",
    "\n",
    "K-means clustering je neřízený algoritmus strojového učení, který se používá k rozdělení souboru dat do $ K $ shluků. Každý datový bod patří do shluku s nejbližší střední hodnotou.\n",
    "\n",
    "Cílem K-means je minimalizovat součet čtverců uvnitř shluku (WCSS), který je definován jako:\n",
    "\n",
    "$$\n",
    "J = \\sum_{i=1}^{K} \\sum_{x \\v C_i} || x - \\mu_i ||^2\n",
    "$$\n",
    "\n",
    "kde $ \\mu_i $ je centroid shluku $ C_i $, $ K $ je počet shluků a $ x $ je datový bod ve shluku $ C_i $.\n",
    "\n",
    "### Kroky algoritmu\n",
    "1. Zvolte $ K $ počátečních centroidů, jeden pro každý shluk. To lze provést náhodně nebo na základě heuristiky.\n",
    "2. Přiřaďte každý datový bod k nejbližšímu centroidu a stane se členem tohoto shluku.\n",
    "3. Vypočítejte nové centroidy jako střední hodnotu všech datových bodů ve shluku.\n",
    "4. Opakujte kroky přiřazení a aktualizace, dokud nedojde k žádnému zlepšení, tj. centroidy se výrazně nezmění."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create synthetic data\n",
    "X, _ = make_blobs(n_samples=300, centers=3, random_state=42, cluster_std=1.0)\n",
    "\n",
    "# Initialize and fit KMeans algorithm\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Fetch centroids and labels\n",
    "centroids = kmeans.cluster_centers_\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Create meshgrid for visualization\n",
    "xx, yy = np.meshgrid(np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 100),\n",
    "                     np.linspace(X[:, 1].min() - 1, X[:, 1].max() + 1, 100))\n",
    "\n",
    "# Predict cluster for meshgrid points\n",
    "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot\n",
    "plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, edgecolors='k', marker='o', linewidth=1)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('K-means Clustering')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analýza hlavních komponent (PCA) \n",
    "\n",
    "Analýza hlavních komponent (PCA) je technika redukce dimenzionality, která se používá k transformaci vysokodimenzionálních dat do méně dimenzionální podoby, přičemž se zachovává co největší část rozptylu původních dat.\n",
    "\n",
    "### Základní matematika\n",
    "\n",
    "Cílem PCA je najít novou sadu os, tzv. hlavních komponent, které maximalizují rozptyl v datech. Matematicky jde o rozklad vlastních čísel kovarianční matice dat $ \\Sigma $:\n",
    "\n",
    "$$\n",
    "\\Sigma V = V D\n",
    "$$\n",
    "\n",
    "Zde $ D $ je diagonální matice vlastních čísel $ \\lambda $ a $ V $ obsahuje odpovídající vlastní vektory $ v $.\n",
    "\n",
    "Hlavní komponenty jsou vlastní vektory seřazené sestupně podle vlastních čísel. První hlavní složka je vlastní vektor spojený s největší vlastní hodnotou a tak dále.\n",
    "\n",
    "Data pak lze do tohoto nového prostoru promítnout pomocí:\n",
    "\n",
    "$$\n",
    "X_{\\text{PCA}} = X V_k\n",
    "$$\n",
    "\n",
    "kde $ V_k $ obsahuje první $ k $ vlastní vektory a $ k $ je počet dimenzí, na které chcete data redukovat.\n",
    "\n",
    "### Kroky algoritmu\n",
    "\n",
    "1. **Centrování dat**: Odečtěte střední hodnotu každého prvku ze souboru dat.\n",
    "2. **Vypočítejte kovarianční matici**: \\( \\Sigma = \\frac{1}{N} X^T X \\)\n",
    "3. **Rozklad vlastních čísel**: Vyřešte \\( V \\) a \\( D \\).\n",
    "4. **Třídění vlastních vektorů**: Seřaďte vlastní vektory podle klesajících vlastních hodnot.\n",
    "5. **Projekce**: Vyberte první \\( k \\) vlastní vektory a promítněte data do nového prostoru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create synthetic data\n",
    "X, y = make_blobs(n_samples=300, centers=3, n_features=2, random_state=42)\n",
    "\n",
    "# Fit PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Plot original data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.title(\"Original Data\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "\n",
    "# Plot PCA-transformed data\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y)\n",
    "plt.title(\"PCA Transformed Data\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V tomto příkladu máme syntetická data ve 2D prostoru, reprezentovaná symbolem `X`. Tato data transformujeme pomocí PCA do nového souřadnicového systému definovaného hlavními komponentami. Protože data jsou již ve 2D, budou transformovaná data také ve 2D.\n",
    "Levý graf ukazuje původní datové body podbarvené jejich skutečnými značkami a pravý graf ukazuje datové body v novém souřadnicovém systému definovaném pomocí PCA.\n",
    "Při pohledu na pravý graf je snadno vidět, že hlavní komponenty (osy) jsou orientovány ve směrech, kde se data nejvíce liší.\n",
    "To by mělo poskytnout solidní pochopení PCA, matematiky, která za ní stojí, a její vizualizace."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
